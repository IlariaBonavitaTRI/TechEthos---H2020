{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3441c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Notebook processes all the raw data stored in the directory \"alldata\" and merge them where necessary.\n",
    "# Make sure to point to the right directory where necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Function to extract the domains\n",
    "def domain_extraction(all_digitalxr):\n",
    "    domain = []\n",
    "    for i in range(len(all_digitalxr)):\n",
    "        ele = all_digitalxr['domain'].iloc[i]\n",
    "        \n",
    "        try:\n",
    "            ele = ele.remove('www')\n",
    "            domain.append(ele[0])\n",
    "        except:\n",
    "            domain.append(  all_digitalxr['domain'].iloc[i][0]   ) \n",
    "            \n",
    "    all_digitalxr['domain'] = domain\n",
    "    return all_digitalxr\n",
    "\n",
    "\n",
    "\n",
    "#Function to extract day and month (arbitrary name used as input data)\n",
    "def day_month_extractor(data_climate_7):\n",
    "    days = []\n",
    "    months = []\n",
    "    for i in range(len(data_climate_7)):\n",
    "        try:\n",
    "            app = [int(s) for s in  data_climate_7['date'][i].split()[0]  if s.isdigit()]\n",
    "        except:\n",
    "            app = [int(s) for s in  data_climate_7['date'].iloc[i].split()[0]  if s.isdigit()]\n",
    "            \n",
    "        s = [str(i) for i in app]\n",
    "        day = int(\"\".join(s))\n",
    "        days.append(day)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            app = [s for s in  data_climate_7['date'][i].split()[1]  if s.lower().isalpha()]\n",
    "        except:\n",
    "            app = [s for s in  data_climate_7['date'].iloc[i].split()[1]  if s.lower().isalpha()]\n",
    "        s = [str(i) for i in app]\n",
    "        month = (\"\".join(s))\n",
    "        months.append(month)\n",
    "        \n",
    "    data_climate_7['day'] = np.array(days)\n",
    "    data_climate_7['month'] = np.array(months)\n",
    "    return data_climate_7\n",
    "  \n",
    "    \n",
    "#Function to convert the month to numbers   \n",
    "def month_numeric(all_climate):\n",
    "    mon_dict = {'Jan':'01', 'Feb':'02', 'Mar':'03', 'Apr':'04','May':'05','Jun':'06','Juni':'06','Jul':'07','Sep':'09','Sept':'09','Aug':'08','Oct':'10','Nov':'11','Dec':'12'}\n",
    "    mon = []\n",
    "    for i in range(len(all_climate)):\n",
    "        try:\n",
    "            if all_climate['month'][i] in list(mon_dict.keys()):\n",
    "                mon.append(mon_dict[ all_climate['month'][i]])\n",
    "            else:\n",
    "                mon.append( all_climate['month'][i] )\n",
    "                \n",
    "        \n",
    "        except:\n",
    "            if all_climate['month'].iloc[i] in list(mon_dict.keys()):\n",
    "                mon.append(mon_dict[ all_climate['month'].iloc[i]])\n",
    "            else:\n",
    "                mon.append(all_climate['month'].iloc[i])\n",
    "    all_climate['month'] = np.array(mon)\n",
    "    return all_climate\n",
    "            \n",
    "        \n",
    "#Function to extract the section names      \n",
    "def get_section_names(all_digitalxr):\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    for j in range(len(all_digitalxr)):\n",
    "        all_section = []\n",
    "        if len(all_digitalxr['section'].iloc[j]) > 1:\n",
    "            for ele in all_digitalxr['section'].iloc[j][1].split(\"/\"):\n",
    "                if ele.isdigit():\n",
    "                    pass\n",
    "                else:\n",
    "                    if len(ele.split(\"-\"))>1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        all_section.append(ele)\n",
    "                        \n",
    "                           \n",
    "        else:\n",
    "            for ele in all_digitalxr['section'].iloc[j][0].split(\"/\"):\n",
    "                if ele.isdigit():\n",
    "                    pass\n",
    "                else:\n",
    "                    if len(ele.split(\"-\"))>1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        all_section.append(ele)\n",
    "\n",
    "        try:\n",
    "            all_section.remove( all_digitalxr['country'].iloc[j] )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #Check if \"\" is present and remoce\n",
    "        try:\n",
    "            all_section.remove(\"\")\n",
    "        except:\n",
    "            pass\n",
    "  \n",
    "        #Remove several domain names\n",
    "        for ele in ['com','net','org','int','edu','gov','mil']:\n",
    "            if ele in all_section:\n",
    "                all_section.remove(ele)\n",
    "                \n",
    "        #Remove more domain names      \n",
    "        for ele in ['rs','de','cz','eu']:\n",
    "            if ele in all_section:\n",
    "                all_section.remove(ele)\n",
    "        \n",
    "        #if list is empty that means the news belong to the main section.\n",
    "        if len(all_section)==0:\n",
    "            all_section.append('main')\n",
    "        \n",
    "        \n",
    "            \n",
    "                                                                           \n",
    "                                                             \n",
    "                                                            \n",
    "            \n",
    "        sections.append(all_section)\n",
    "    return sections\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0095646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Data from Climate Keyword.\n",
    "\n",
    "\n",
    "#Read Data and use pandas and split function to extract day, month and year info. from date.\n",
    "data_climate_1 = pd.read_csv('alldata/Gnewsclimate.csv')\n",
    "data_climate_1['year'] = data_climate_1['date'].str.split(\"-\").str[0]\n",
    "data_climate_1['month']  = data_climate_1['date'].str.split(\"-\").str[1]\n",
    "data_climate_1['day'] = data_climate_1['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_climate_2 = pd.read_csv('alldata/v3/Gnewsclimate.csv')\n",
    "data_climate_2['year'] = data_climate_2['date'].str.split(\"-\").str[0]\n",
    "data_climate_2['month']  = data_climate_2['date'].str.split(\"-\").str[1]\n",
    "data_climate_2['day'] = data_climate_2['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_climate_3 = pd.read_csv('alldata/v3-2019/Gnewsclimate.csv')\n",
    "data_climate_3['year'] = data_climate_3['date'].str.split(\"-\").str[0]\n",
    "data_climate_3['month']  = data_climate_3['date'].str.split(\"-\").str[1]\n",
    "data_climate_3['day'] = data_climate_3['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_climate_4 = pd.read_csv('alldata/v3-2020/Gnewsclimate.csv')\n",
    "data_climate_4['year'] = data_climate_4['date'].str.split(\"-\").str[0]\n",
    "data_climate_4['month']  = data_climate_4['date'].str.split(\"-\").str[1]\n",
    "data_climate_4['day'] = data_climate_4['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_climate_5 = pd.read_csv('alldata/2019-2021/Gnewsclimate.csv')\n",
    "data_climate_5['year'] = data_climate_5['date'].str.split(\"-\").str[0]\n",
    "data_climate_5['month']  = data_climate_5['date'].str.split(\"-\").str[1]\n",
    "data_climate_5['day'] = data_climate_5['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_climate_6 = pd.read_csv('alldata/serbia_and_czech_google_news/new_processed_climate.csv')\n",
    "data_climate_6 = data_climate_6[data_climate_6.columns[0:7]] \n",
    "data_climate_6['month']  = data_climate_6['date'].str.split(\",\").str[1].str.split().str[1]\n",
    "data_climate_6['day'] = data_climate_6['date'].str.split(\",\").str[1].str.split().str[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years = ['2019','2020','2021']\n",
    "for i, year in enumerate(years):\n",
    "    if i == 0:\n",
    "        data_climate_6_ = data_climate_6[data_climate_6['year']==year]\n",
    "    else:\n",
    "        data_climate_6_ = pd.concat([data_climate_6_ ,  data_climate_6[data_climate_6['year']==year]])\n",
    "data_climate_6 = data_climate_6_        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_climate_7 = pd.read_csv('alldata/Austria/Austria_climate_v2.csv')\n",
    "data_climate_7 = data_climate_7[data_climate_7.columns[0:8]]#Select relevant data\n",
    "data_climate_7 = data_climate_7.dropna()#Drop entries with missing data.\n",
    "data_climate_7['country'] = np.array(len(data_climate_7)*['at'])#Add country information\n",
    "data_climate_7 = data_climate_7.drop(columns=['position','Unnamed: 0.1'])\n",
    "data_climate_7 = data_climate_7.rename(columns={\"link\": \"url\"})#rename column for uniformity\n",
    "data_climate_7 = data_climate_7[data_climate_7['url'].str.startswith('http')]#Select right url \n",
    "data_climate_7 = day_month_extractor(data_climate_7)#Extracts date and month using the function day_month_extractor\n",
    "data_climate_7 = data_climate_7[ data_climate_1.columns.tolist()  ]# Add column names\n",
    "\n",
    "\n",
    "\n",
    "data_climate_8 = pd.read_csv('alldata/Austria/Austria_climate.csv')\n",
    "data_climate_8 = data_climate_8[data_climate_8.columns[0:8]]#Select relevant data\n",
    "data_climate_8 = data_climate_8.dropna()#drop entries with missing data\n",
    "data_climate_8['country'] = np.array(len(data_climate_8)*['at'])# Add country information\n",
    "data_climate_8 = data_climate_8.drop(columns=['position','Unnamed: 0.1'])#Drop irrelevant columns.\n",
    "data_climate_8 = data_climate_8.rename(columns={\"link\": \"url\"})#rename column for uniformity\n",
    "data_climate_8 = data_climate_8[data_climate_8['url'].str.startswith('http')]#Select right url \n",
    "data_climate_8 = day_month_extractor(data_climate_8)#Extracts date and month using the function day_month_extractor\n",
    "data_climate_8 = data_climate_8[ data_climate_1.columns.tolist()  ]# Add column names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Combine all processed data above together vertically\n",
    "\n",
    "\n",
    "all_climate = data_climate_2\n",
    "all_climate = pd.concat([all_climate, data_climate_3])\n",
    "all_climate = pd.concat([all_climate, data_climate_4])\n",
    "all_climate = pd.concat([all_climate, data_climate_5])\n",
    "all_climate = pd.concat([all_climate, data_climate_6])\n",
    "all_climate = pd.concat([all_climate, data_climate_7])\n",
    "all_climate = pd.concat([all_climate, data_climate_8])\n",
    "\n",
    "\n",
    "\n",
    "all_climate['year'] = all_climate['year'].astype(int)#Convert the year column to data type integer\n",
    "#Remove year 2019\n",
    "all_climate = all_climate[all_climate['year']>2019]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove url\n",
    "all_climate = all_climate.drop_duplicates(subset=['url'])#drop dupicate data using the urls\n",
    "\n",
    "#Get candidate list of potential domains.\n",
    "all_climate['domain'] = all_climate['url'].str.split(\"//\").str[1].str.split(\".\").str[0:2]\n",
    "all_climate = all_climate.dropna()# drop rows with at least one empty cell\n",
    "all_climate = domain_extraction(all_climate)#Extracts domain name using the function domain_extraction\n",
    "\n",
    "\n",
    "#remove year\n",
    "all_climate['year'] = all_climate['year'].astype(int)# Convert column to data type integers\n",
    "all_climate = all_climate[all_climate['year']>2019] # Remove year 2019\n",
    "all_climate = all_climate.dropna()# drop rows with at least one empty cell\n",
    "all_climate = month_numeric(all_climate) # Converts all data in month column to numbers.\n",
    "\n",
    "all_climate['section'] = all_climate['url'].str.split(\".\").str[1:]\n",
    "all_climate['section'] = get_section_names(all_climate)#GET Section names using the function get_section_names\n",
    "all_climate.to_csv('climate.csv')# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6672627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Data from Neurotech Keyword.\n",
    "\n",
    "\n",
    "data_neurotech_1 = pd.read_csv('alldata/Gnewsneurotech.csv')\n",
    "data_neurotech_1['year'] = data_neurotech_1['date'].str.split(\"-\").str[0]\n",
    "data_neurotech_1['month']  = data_neurotech_1['date'].str.split(\"-\").str[1]\n",
    "data_neurotech_1['day'] = data_neurotech_1['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "data_neurotech_2 = pd.read_csv('alldata/v3/Gnewsneurotech.csv')\n",
    "data_neurotech_2['year'] = data_neurotech_2['date'].str.split(\"-\").str[0]\n",
    "data_neurotech_2['month']  = data_neurotech_2['date'].str.split(\"-\").str[1]\n",
    "data_neurotech_2['day'] = data_neurotech_2['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "data_neurotech_3 = pd.read_csv('alldata/v3-2019/Gnewsneurotech.csv')\n",
    "data_neurotech_3['year'] = data_neurotech_3['date'].str.split(\"-\").str[0]\n",
    "data_neurotech_3['month']  = data_neurotech_3['date'].str.split(\"-\").str[1]\n",
    "data_neurotech_3['day'] = data_neurotech_3['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_neurotech_4 = pd.read_csv('alldata/v3-2020/Gnewsneurotech.csv')\n",
    "data_neurotech_4['year'] = data_neurotech_4['date'].str.split(\"-\").str[0]\n",
    "data_neurotech_4['month']  = data_neurotech_4['date'].str.split(\"-\").str[1]\n",
    "data_neurotech_4['day'] = data_neurotech_4['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_neurotech_5 = pd.read_csv('alldata/2019-2021/Gnewsneurotech.csv')\n",
    "data_neurotech_5['year'] = data_neurotech_5['date'].str.split(\"-\").str[0]\n",
    "data_neurotech_5['month']  = data_neurotech_5['date'].str.split(\"-\").str[1]\n",
    "data_neurotech_5['day'] = data_neurotech_5['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_neurotech_6 = pd.read_csv('alldata/serbia_and_czech_google_news/new_processed_neurotech.csv')\n",
    "data_neurotech_6 = data_neurotech_6[data_neurotech_6.columns[0:7]] \n",
    "data_neurotech_6['month']  = data_neurotech_6['date'].str.split(\",\").str[1].str.split().str[1]\n",
    "data_neurotech_6['day'] = data_neurotech_6['date'].str.split(\",\").str[1].str.split().str[0]\n",
    "\n",
    "\n",
    "data_neurotech_7 = pd.read_csv('alldata/Austria/Austria_neurotech_v2.csv')\n",
    "data_neurotech_7 = data_neurotech_7[data_neurotech_7.columns[0:8]]#Selects relevant data.\n",
    "data_neurotech_7 = data_neurotech_7.dropna()#drop entries with missing data\n",
    "data_neurotech_7['country'] = np.array(len(data_neurotech_7)*['at'])#Add country information.\n",
    "data_neurotech_7 = data_neurotech_7.drop(columns=['position','Unnamed: 0.1'])#Drop irrelevant columns\n",
    "data_neurotech_7 = data_neurotech_7.rename(columns={\"link\": \"url\"})#rename link to url for uniformity\n",
    "data_neurotech_7 = data_neurotech_7[data_neurotech_7['url'].str.startswith('http')]#Select right url \n",
    "data_neurotech_7 = day_month_extractor(data_neurotech_7)#Extracts date and month using the function day_month_extractor\n",
    "data_neurotech_7 = data_neurotech_7[ data_neurotech_1.columns.tolist()  ]# Add column names\n",
    "\n",
    "\n",
    "\n",
    "data_neurotech_8 = pd.read_csv('alldata/Austria/Austria_neurotech.csv')\n",
    "data_neurotech_8 = data_neurotech_8[data_neurotech_8.columns[0:8]]#Selects relevant data.\n",
    "data_neurotech_8 = data_neurotech_8.dropna()#drop entries with missing data\n",
    "data_neurotech_8['country'] = np.array(len(data_neurotech_8)*['at'])#Add country information.\n",
    "data_neurotech_8 = data_neurotech_8.drop(columns=['position','Unnamed: 0.1'])#Drop irrelevant columns\n",
    "data_neurotech_8 = data_neurotech_8.rename(columns={\"link\": \"url\"})#rename link to url for uniformity\n",
    "data_neurotech_8 = data_neurotech_8[data_neurotech_8['url'].str.startswith('http')]#Select right url \n",
    "data_neurotech_8 = day_month_extractor(data_neurotech_8)#Extracts date and month using the function day_month_extractor\n",
    "data_neurotech_8 = data_neurotech_8[ data_neurotech_1.columns.tolist()  ]# Add column names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Combine all processed data above together vertically.\n",
    "\n",
    "all_neurotech = data_neurotech_2\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_3])\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_4])\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_5])\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_6])\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_7])\n",
    "all_neurotech = pd.concat([all_neurotech, data_neurotech_8])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove url\n",
    "all_neurotech = all_neurotech.drop_duplicates(subset=['url'])#drop duplicate data using the urls\n",
    "\n",
    "#Get candidate list of potential domains.\n",
    "all_neurotech['domain'] = all_neurotech['url'].str.split(\"//\").str[1].str.split(\".\").str[0:2]\n",
    "all_neurotech = all_neurotech.dropna()# drop rows with at least one empty cell\n",
    "all_neurotech = domain_extraction(all_neurotech)#Extracts domain name using the function domain_extraction\n",
    "\n",
    "\n",
    "#remove year\n",
    "all_neurotech['year'] = all_neurotech['year'].astype(int)# Convert column to data type integers\n",
    "all_neurotech = all_neurotech[all_neurotech['year']>2019]# Remove year 2019\n",
    "all_neurotech = all_neurotech.dropna()# drop rows with at least one empty cell\n",
    "all_neurotech = month_numeric(all_neurotech)\n",
    "\n",
    "\n",
    "all_neurotech['section'] = all_neurotech['url'].str.split(\".\").str[1:]\n",
    "all_neurotech['section'] = get_section_names(all_neurotech)#GET Section names using the function get_section_names\n",
    "all_neurotech.to_csv('neurotech.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd2ce81",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-50ed0cabfb5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mall_digitalxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_digitalxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_digitalxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# Remove year 2019\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mall_digitalxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_digitalxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# drop rows with at least one empty cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mall_digitalxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_digitalxr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_digitalxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mall_digitalxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_digitalxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "# Processing Data from Digitalxr Keyword.\n",
    "\n",
    "\n",
    "\n",
    "data_digitalxr_1 = pd.read_csv('alldata/Gnewsdigitalxr.csv')\n",
    "data_digitalxr_1['year'] = data_digitalxr_1['date'].str.split(\"-\").str[0]\n",
    "data_digitalxr_1['month']  = data_digitalxr_1['date'].str.split(\"-\").str[1]\n",
    "data_digitalxr_1['day'] = data_digitalxr_1['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_digitalxr_2 = pd.read_csv('alldata/v3/Gnewsdigitalxr.csv')\n",
    "data_digitalxr_2['year'] = data_digitalxr_2['date'].str.split(\"-\").str[0]\n",
    "data_digitalxr_2['month']  = data_digitalxr_2['date'].str.split(\"-\").str[1]\n",
    "data_digitalxr_2['day'] = data_digitalxr_2['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "data_digitalxr_3 = pd.read_csv('alldata/v3-2019/Gnewsdigitalxr.csv')\n",
    "data_digitalxr_3['year'] = data_digitalxr_3['date'].str.split(\"-\").str[0]\n",
    "data_digitalxr_3['month']  = data_digitalxr_3['date'].str.split(\"-\").str[1]\n",
    "data_digitalxr_3['day'] = data_digitalxr_3['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_digitalxr_4 = pd.read_csv('alldata/v3-2020/Gnewsdigitalxr.csv')\n",
    "data_digitalxr_4['year'] = data_digitalxr_4['date'].str.split(\"-\").str[0]\n",
    "data_digitalxr_4['month']  = data_digitalxr_4['date'].str.split(\"-\").str[1]\n",
    "data_digitalxr_4['day'] = data_digitalxr_4['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "data_digitalxr_5 = pd.read_csv('alldata/2019-2021/Gnewsdigitalxr.csv')\n",
    "data_digitalxr_5['year'] = data_digitalxr_5['date'].str.split(\"-\").str[0]\n",
    "data_digitalxr_5['month']  = data_digitalxr_5['date'].str.split(\"-\").str[1]\n",
    "data_digitalxr_5['day'] = data_digitalxr_5['date'].str.split(\"-\").str[2].str.split(\"T\").str[0]\n",
    "\n",
    "\n",
    "data_digitalxr_6 = pd.read_csv('alldata/serbia_and_czech_google_news/new_processed_digitalxr.csv')\n",
    "data_digitalxr_6 = data_digitalxr_6[data_digitalxr_6.columns[0:7]] \n",
    "data_digitalxr_6['month']  = data_digitalxr_6['date'].str.split(\",\").str[1].str.split().str[1]\n",
    "data_digitalxr_6['day'] = data_digitalxr_6['date'].str.split(\",\").str[1].str.split().str[0]\n",
    "\n",
    "\n",
    "\n",
    "data_digitalxr_7 = pd.read_csv('alldata/Austria/Austria_digitalxr_v2.csv')\n",
    "data_digitalxr_7 = data_digitalxr_7[data_digitalxr_7.columns[0:8]]#Select relevant columns\n",
    "data_digitalxr_7 = data_digitalxr_7.dropna()#drop entries with missing data\n",
    "data_digitalxr_7['country'] = np.array(len(data_digitalxr_7)*['at'])#Add the country information\n",
    "data_digitalxr_7 = data_digitalxr_7.drop(columns=['position','Unnamed: 0.1'])#Drop irrelevant columns\n",
    "data_digitalxr_7 = data_digitalxr_7.rename(columns={\"link\": \"url\"})#rename link to url for uniformity\n",
    "data_digitalxr_7 = data_digitalxr_7[data_digitalxr_7['url'].str.startswith('http')]#Select right url \n",
    "data_digitalxr_7 = day_month_extractor(data_digitalxr_7)#Extracts date and month using the function day_month_extractor\n",
    "data_digitalxr_7 = data_digitalxr_7[ data_digitalxr_1.columns.tolist()  ] # Add column names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_digitalxr_8 = pd.read_csv('alldata/Austria/Austria_digitalxr.csv')\n",
    "data_digitalxr_8 = data_digitalxr_8[data_digitalxr_8.columns[0:8]]#Select relevant columns\n",
    "data_digitalxr_8 = data_digitalxr_8.dropna()#drop entries with missing data\n",
    "data_digitalxr_8['country'] = np.array(len(data_digitalxr_8)*['at'])#Add the country information\n",
    "data_digitalxr_8 = data_digitalxr_8.drop(columns=['position','Unnamed: 0.1'])#Drop irrelevant columns\n",
    "data_digitalxr_8 = data_digitalxr_8.rename(columns={\"link\": \"url\"})#rename link to url for uniformity\n",
    "data_digitalxr_8 = data_digitalxr_8[data_digitalxr_8['url'].str.startswith('http')]#Select right url \n",
    "data_digitalxr_8 = day_month_extractor(data_digitalxr_8)#Extracts date and month using the function day_month_extractor\n",
    "data_digitalxr_8 = data_digitalxr_8[ data_digitalxr_1.columns.tolist()  ]# Add column names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Combine all processed data above together vertically\n",
    "\n",
    "#all_digitalxr = data_digitalxr_1\n",
    "#all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_2])\n",
    "\n",
    "all_digitalxr = data_digitalxr_2\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_3])\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_4])\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_5])\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_6])\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_7])\n",
    "all_digitalxr = pd.concat([all_digitalxr, data_digitalxr_8])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove url\n",
    "all_digitalxr = all_digitalxr.drop_duplicates(subset=['url'])#drop duplicate entries using the urls\n",
    "\n",
    "#Get candidate list of potential domains.\n",
    "all_digitalxr['domain'] = all_digitalxr['url'].str.split(\"//\").str[1].str.split(\".\").str[0:2]\n",
    "\n",
    "all_digitalxr = all_digitalxr.dropna()# Drop rows with at least one empty cell.\n",
    "all_digitalxr = domain_extraction(all_digitalxr)#Extracts domain name using the function domain_extraction.\n",
    "\n",
    "\n",
    "#remove year 2019\n",
    "all_digitalxr['year'] = all_digitalxr['year'].astype(int) # Make column to be of data type integer\n",
    "all_digitalxr = all_digitalxr[all_digitalxr['year']>2019]# Remove year 2019\n",
    "all_digitalxr = all_digitalxr.dropna()# drop rows with at least one empty cell\n",
    "all_digitalxr = all_digitalxr(all_digitalxr)\n",
    "\n",
    "all_digitalxr['section'] = all_digitalxr['url'].str.split(\".\").str[1:]\n",
    "all_digitalxr['section'] = get_section_names(all_digitalxr)#gET Section names using the function get_section_names\n",
    "all_digitalxr.to_csv('digitalxr.csv')#Save data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3aae7",
   "metadata": {},
   "source": [
    "list(all_digitalxr['domain'].iloc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
